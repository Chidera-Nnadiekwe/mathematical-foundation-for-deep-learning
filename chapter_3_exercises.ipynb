{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9dd64e7-9627-47d2-b16b-4978468019a5",
   "metadata": {},
   "source": [
    "# Chapter 3 Exercises\n",
    "-----\n",
    "## 1. Hands-on Example\n",
    "-----\n",
    "The hands-on demonstration shows how multivariate calculus is applied in deep learning, specifically in gradient descent optimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51ad807-1b24-4a63-b6f8-9a7696809fa9",
   "metadata": {},
   "source": [
    "### Import needed libraries\n",
    "- The ```Numpy``` library is imported as ```np``` (for arrays and matrix computation and manipulation)\n",
    "- The ```matplotlib.pyplot``` module is imported as ```plt``` (for static, interactive, and animated visualization)\n",
    "- ```Axes3D``` is imported from ```mpl_toolkits.mplot3d``` 3D plot creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "928875a8-ecf3-4853-b439-ac29c1d89ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5838d52-9370-47bc-b834-d9022471d153",
   "metadata": {},
   "source": [
    "### Define the multivariate function\n",
    "- The following two-variable quadratic function will be used: $$f(x,y) = x^2 + y^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ee0d95f-395a-4c64-b99c-0e92e4b84f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the multivariate function\n",
    "def f(x,y):\n",
    "    z = (x**2 + y**2)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d18e64e-fac4-44a7-9264-f5fc77227747",
   "metadata": {},
   "source": [
    "### Compute the gradient\n",
    "- The partial derivative of a function with respect to variables gives the gradient of that function as shown:  $$\\nabla f = \\left ( \\frac{\\partial f}{\\partial x},\\ \\frac{\\partial f}{\\partial y}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0fd3fe6-bff5-4773-a15e-59108e833ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the gradient of the function\n",
    "def gradient(x,y):\n",
    "    df_dx = 2 * x\n",
    "    df_dy = 2 * y\n",
    "    result = np.array([df_dx, df_dy])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed25c165-f108-4fcf-b166-e2c766c82ed7",
   "metadata": {},
   "source": [
    "### Gradient Descent Implementation\n",
    "- Implement a simple version of the gradient descent optimization algorithm.\n",
    "- The ```gradient_descent``` function requires ```starting_point```, ```learning_rate```, and ```iterations```.\n",
    "- ```learning_rate``` controls how much the ```updated_points``` move in the opposite direction of the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfef2175-6ad0-4ff9-80b8-63b8221bba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(starting_point, learning_rate, iterations):\n",
    "    points = [starting_point]\n",
    "    point = starting_point\n",
    "    for i in range(iterations):\n",
    "        grad = gradient(point[0], point[1])\n",
    "        point = point - learning_rate * grad\n",
    "        points.append(point)\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c7c454-c809-4350-a943-6f96064c57c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
